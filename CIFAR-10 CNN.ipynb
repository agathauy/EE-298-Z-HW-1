{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5747
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5379374,
     "status": "ok",
     "timestamp": 1551892277005,
     "user": {
      "displayName": "Agatha Uy",
      "photoUrl": "",
      "userId": "16915807624742537561"
     },
     "user_tz": -480
    },
    "id": "TA5UaK2HmzFs",
    "outputId": "f3723bd3-5bf4-44d7-9676-338f455d86b0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 413,578\n",
      "Trainable params: 412,682\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 2.1479 - acc: 0.4361 - val_loss: 1.7033 - val_acc: 0.5064\n",
      "Epoch 2/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 1.5170 - acc: 0.5731 - val_loss: 1.1521 - val_acc: 0.6745\n",
      "Epoch 3/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 1.3869 - acc: 0.6229 - val_loss: 1.1901 - val_acc: 0.6679\n",
      "Epoch 4/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 1.3468 - acc: 0.6511 - val_loss: 1.2687 - val_acc: 0.6595\n",
      "Epoch 5/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 1.2873 - acc: 0.6654 - val_loss: 1.1448 - val_acc: 0.7159\n",
      "Epoch 6/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 1.2303 - acc: 0.6821 - val_loss: 1.0060 - val_acc: 0.7359\n",
      "Epoch 7/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 1.2084 - acc: 0.6961 - val_loss: 1.0558 - val_acc: 0.7252\n",
      "Epoch 8/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 1.1858 - acc: 0.7044 - val_loss: 0.9865 - val_acc: 0.7465\n",
      "Epoch 9/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 1.1636 - acc: 0.7121 - val_loss: 0.9935 - val_acc: 0.7468\n",
      "Epoch 10/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 1.1579 - acc: 0.7181 - val_loss: 1.1248 - val_acc: 0.7265\n",
      "Epoch 11/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 1.1268 - acc: 0.7288 - val_loss: 0.9274 - val_acc: 0.7651\n",
      "Epoch 12/125\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 1.1010 - acc: 0.7342 - val_loss: 0.8860 - val_acc: 0.7711\n",
      "Epoch 13/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 1.0925 - acc: 0.7381 - val_loss: 0.9218 - val_acc: 0.7600\n",
      "Epoch 14/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 1.0784 - acc: 0.7430 - val_loss: 0.9660 - val_acc: 0.7575\n",
      "Epoch 15/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 1.0727 - acc: 0.7442 - val_loss: 0.8839 - val_acc: 0.7696\n",
      "Epoch 16/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 1.0574 - acc: 0.7495 - val_loss: 0.8861 - val_acc: 0.7749\n",
      "Epoch 17/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 1.0563 - acc: 0.7539 - val_loss: 0.8533 - val_acc: 0.7909\n",
      "Epoch 18/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 1.0416 - acc: 0.7546 - val_loss: 0.9389 - val_acc: 0.7703\n",
      "Epoch 19/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 1.0236 - acc: 0.7602 - val_loss: 0.9500 - val_acc: 0.7749\n",
      "Epoch 20/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 1.0108 - acc: 0.7603 - val_loss: 0.8827 - val_acc: 0.7812\n",
      "Epoch 21/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.9973 - acc: 0.7630 - val_loss: 0.8379 - val_acc: 0.8060\n",
      "Epoch 22/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.9845 - acc: 0.7657 - val_loss: 0.8791 - val_acc: 0.7750\n",
      "Epoch 23/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.9682 - acc: 0.7676 - val_loss: 0.8107 - val_acc: 0.8006\n",
      "Epoch 24/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.9585 - acc: 0.7706 - val_loss: 0.8108 - val_acc: 0.7979\n",
      "Epoch 25/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.9373 - acc: 0.7739 - val_loss: 0.8316 - val_acc: 0.7996\n",
      "Epoch 26/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.9220 - acc: 0.7744 - val_loss: 0.7890 - val_acc: 0.8083\n",
      "Epoch 27/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.8921 - acc: 0.7765 - val_loss: 0.7816 - val_acc: 0.8092\n",
      "Epoch 28/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.8663 - acc: 0.7810 - val_loss: 0.7990 - val_acc: 0.8026\n",
      "Epoch 29/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.8359 - acc: 0.7834 - val_loss: 0.7466 - val_acc: 0.8155\n",
      "Epoch 30/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.7980 - acc: 0.7878 - val_loss: 0.7562 - val_acc: 0.8046\n",
      "Epoch 31/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.7764 - acc: 0.7903 - val_loss: 0.9127 - val_acc: 0.7936\n",
      "Epoch 32/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7633 - acc: 0.7924 - val_loss: 0.7200 - val_acc: 0.8192\n",
      "Epoch 33/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7544 - acc: 0.7972 - val_loss: 0.7308 - val_acc: 0.8126\n",
      "Epoch 34/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7494 - acc: 0.7982 - val_loss: 0.7222 - val_acc: 0.8114\n",
      "Epoch 35/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.7396 - acc: 0.8006 - val_loss: 0.6955 - val_acc: 0.8199\n",
      "Epoch 36/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7384 - acc: 0.7989 - val_loss: 0.6578 - val_acc: 0.8326\n",
      "Epoch 37/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7259 - acc: 0.8030 - val_loss: 0.6770 - val_acc: 0.8252\n",
      "Epoch 38/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7304 - acc: 0.8007 - val_loss: 0.7191 - val_acc: 0.8198\n",
      "Epoch 39/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7201 - acc: 0.8044 - val_loss: 0.6850 - val_acc: 0.8220\n",
      "Epoch 40/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7216 - acc: 0.8043 - val_loss: 0.6782 - val_acc: 0.8232\n",
      "Epoch 41/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.7191 - acc: 0.8048 - val_loss: 0.6807 - val_acc: 0.8233\n",
      "Epoch 42/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7139 - acc: 0.8055 - val_loss: 0.6956 - val_acc: 0.8212\n",
      "Epoch 43/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7145 - acc: 0.8054 - val_loss: 0.7045 - val_acc: 0.8147\n",
      "Epoch 44/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7144 - acc: 0.8062 - val_loss: 0.6443 - val_acc: 0.8362\n",
      "Epoch 45/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7053 - acc: 0.8087 - val_loss: 0.6277 - val_acc: 0.8404\n",
      "Epoch 46/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7095 - acc: 0.8080 - val_loss: 0.6834 - val_acc: 0.8207\n",
      "Epoch 47/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7046 - acc: 0.8103 - val_loss: 0.7166 - val_acc: 0.8149\n",
      "Epoch 48/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7021 - acc: 0.8102 - val_loss: 0.6288 - val_acc: 0.8371\n",
      "Epoch 49/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6997 - acc: 0.8124 - val_loss: 0.6754 - val_acc: 0.8245\n",
      "Epoch 50/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6991 - acc: 0.8100 - val_loss: 0.7190 - val_acc: 0.8127\n",
      "Epoch 51/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6962 - acc: 0.8133 - val_loss: 0.7509 - val_acc: 0.8071\n",
      "Epoch 52/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6969 - acc: 0.8117 - val_loss: 0.6833 - val_acc: 0.8280\n",
      "Epoch 53/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6972 - acc: 0.8133 - val_loss: 0.6480 - val_acc: 0.8344\n",
      "Epoch 54/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6909 - acc: 0.8145 - val_loss: 0.6493 - val_acc: 0.8296\n",
      "Epoch 55/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6856 - acc: 0.8164 - val_loss: 0.6456 - val_acc: 0.8374\n",
      "Epoch 56/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6887 - acc: 0.8133 - val_loss: 0.7145 - val_acc: 0.8112\n",
      "Epoch 57/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6874 - acc: 0.8166 - val_loss: 0.6449 - val_acc: 0.8392\n",
      "Epoch 58/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6821 - acc: 0.8186 - val_loss: 0.6690 - val_acc: 0.8248\n",
      "Epoch 59/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6849 - acc: 0.8171 - val_loss: 0.6120 - val_acc: 0.8471\n",
      "Epoch 60/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6880 - acc: 0.8147 - val_loss: 0.6692 - val_acc: 0.8273\n",
      "Epoch 61/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6870 - acc: 0.8177 - val_loss: 0.6403 - val_acc: 0.8348\n",
      "Epoch 62/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6860 - acc: 0.8164 - val_loss: 0.6502 - val_acc: 0.8304\n",
      "Epoch 63/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6833 - acc: 0.8175 - val_loss: 0.6531 - val_acc: 0.8306\n",
      "Epoch 64/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6823 - acc: 0.8177 - val_loss: 0.6630 - val_acc: 0.8322\n",
      "Epoch 65/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6800 - acc: 0.8187 - val_loss: 0.6608 - val_acc: 0.8325\n",
      "Epoch 66/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6835 - acc: 0.8177 - val_loss: 0.6460 - val_acc: 0.8410\n",
      "Epoch 67/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6810 - acc: 0.8205 - val_loss: 0.7011 - val_acc: 0.8179\n",
      "Epoch 68/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6777 - acc: 0.8171 - val_loss: 0.6169 - val_acc: 0.8428\n",
      "Epoch 69/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6754 - acc: 0.8197 - val_loss: 0.6416 - val_acc: 0.8359\n",
      "Epoch 70/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6744 - acc: 0.8216 - val_loss: 0.6332 - val_acc: 0.8399\n",
      "Epoch 71/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6725 - acc: 0.8184 - val_loss: 0.6356 - val_acc: 0.8421\n",
      "Epoch 72/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6734 - acc: 0.8207 - val_loss: 0.6659 - val_acc: 0.8277\n",
      "Epoch 73/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6763 - acc: 0.8196 - val_loss: 0.6561 - val_acc: 0.8341\n",
      "Epoch 74/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6673 - acc: 0.8226 - val_loss: 0.6273 - val_acc: 0.8430\n",
      "Epoch 75/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6713 - acc: 0.8216 - val_loss: 0.6398 - val_acc: 0.8377\n",
      "Epoch 76/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6708 - acc: 0.8205 - val_loss: 0.6128 - val_acc: 0.8425\n",
      "Epoch 77/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6193 - acc: 0.8378 - val_loss: 0.5661 - val_acc: 0.8587\n",
      "Epoch 78/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6063 - acc: 0.8418 - val_loss: 0.5627 - val_acc: 0.8626\n",
      "Epoch 79/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5964 - acc: 0.8441 - val_loss: 0.5573 - val_acc: 0.8589\n",
      "Epoch 80/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5829 - acc: 0.8483 - val_loss: 0.5635 - val_acc: 0.8606\n",
      "Epoch 81/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5836 - acc: 0.8455 - val_loss: 0.5540 - val_acc: 0.8628\n",
      "Epoch 82/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5764 - acc: 0.8475 - val_loss: 0.5501 - val_acc: 0.8597\n",
      "Epoch 83/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5741 - acc: 0.8481 - val_loss: 0.5652 - val_acc: 0.8586\n",
      "Epoch 84/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5617 - acc: 0.8500 - val_loss: 0.5173 - val_acc: 0.8687\n",
      "Epoch 85/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5598 - acc: 0.8507 - val_loss: 0.5871 - val_acc: 0.8466\n",
      "Epoch 86/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5595 - acc: 0.8503 - val_loss: 0.5628 - val_acc: 0.8524\n",
      "Epoch 87/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5603 - acc: 0.8499 - val_loss: 0.5397 - val_acc: 0.8648\n",
      "Epoch 88/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5541 - acc: 0.8528 - val_loss: 0.5240 - val_acc: 0.8648\n",
      "Epoch 89/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5518 - acc: 0.8538 - val_loss: 0.5278 - val_acc: 0.8626\n",
      "Epoch 90/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5529 - acc: 0.8512 - val_loss: 0.5441 - val_acc: 0.8601\n",
      "Epoch 91/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5548 - acc: 0.8500 - val_loss: 0.5231 - val_acc: 0.8649\n",
      "Epoch 92/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5476 - acc: 0.8533 - val_loss: 0.5261 - val_acc: 0.8622\n",
      "Epoch 93/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5404 - acc: 0.8543 - val_loss: 0.5431 - val_acc: 0.8608\n",
      "Epoch 94/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5494 - acc: 0.8510 - val_loss: 0.5182 - val_acc: 0.8654\n",
      "Epoch 95/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5393 - acc: 0.8518 - val_loss: 0.5458 - val_acc: 0.8569\n",
      "Epoch 96/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5404 - acc: 0.8552 - val_loss: 0.5159 - val_acc: 0.8631\n",
      "Epoch 97/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5462 - acc: 0.8515 - val_loss: 0.5133 - val_acc: 0.8662\n",
      "Epoch 98/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5392 - acc: 0.8532 - val_loss: 0.5454 - val_acc: 0.8568\n",
      "Epoch 99/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5388 - acc: 0.8543 - val_loss: 0.5719 - val_acc: 0.8497\n",
      "Epoch 100/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5339 - acc: 0.8541 - val_loss: 0.5358 - val_acc: 0.8629\n",
      "Epoch 101/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5312 - acc: 0.8552 - val_loss: 0.5369 - val_acc: 0.8595\n",
      "Epoch 102/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5336 - acc: 0.8546 - val_loss: 0.5314 - val_acc: 0.8591\n",
      "Epoch 103/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5322 - acc: 0.8539 - val_loss: 0.5173 - val_acc: 0.8634\n",
      "Epoch 104/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5351 - acc: 0.8531 - val_loss: 0.5255 - val_acc: 0.8615\n",
      "Epoch 105/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5316 - acc: 0.8551 - val_loss: 0.5612 - val_acc: 0.8537\n",
      "Epoch 106/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5269 - acc: 0.8579 - val_loss: 0.5829 - val_acc: 0.8507\n",
      "Epoch 107/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5333 - acc: 0.8551 - val_loss: 0.5241 - val_acc: 0.8649\n",
      "Epoch 108/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.5313 - acc: 0.8564 - val_loss: 0.5536 - val_acc: 0.8504\n",
      "Epoch 109/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.5273 - acc: 0.8571 - val_loss: 0.5250 - val_acc: 0.8627\n",
      "Epoch 110/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5310 - acc: 0.8558 - val_loss: 0.5068 - val_acc: 0.8695\n",
      "Epoch 111/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5250 - acc: 0.8564 - val_loss: 0.5469 - val_acc: 0.8548\n",
      "Epoch 112/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5215 - acc: 0.8581 - val_loss: 0.5275 - val_acc: 0.8595\n",
      "Epoch 113/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5289 - acc: 0.8565 - val_loss: 0.5330 - val_acc: 0.8598\n",
      "Epoch 114/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5253 - acc: 0.8574 - val_loss: 0.4877 - val_acc: 0.8712\n",
      "Epoch 115/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5254 - acc: 0.8566 - val_loss: 0.5667 - val_acc: 0.8492\n",
      "Epoch 116/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5259 - acc: 0.8575 - val_loss: 0.5514 - val_acc: 0.8523\n",
      "Epoch 117/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5206 - acc: 0.8585 - val_loss: 0.5544 - val_acc: 0.8550\n",
      "Epoch 118/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5215 - acc: 0.8569 - val_loss: 0.5106 - val_acc: 0.8683\n",
      "Epoch 119/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5226 - acc: 0.8584 - val_loss: 0.5130 - val_acc: 0.8659\n",
      "Epoch 120/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5230 - acc: 0.8586 - val_loss: 0.5227 - val_acc: 0.8632\n",
      "Epoch 121/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5249 - acc: 0.8575 - val_loss: 0.5547 - val_acc: 0.8534\n",
      "Epoch 122/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5194 - acc: 0.8598 - val_loss: 0.5123 - val_acc: 0.8645\n",
      "Epoch 123/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5175 - acc: 0.8597 - val_loss: 0.5035 - val_acc: 0.8654\n",
      "Epoch 124/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5214 - acc: 0.8578 - val_loss: 0.4932 - val_acc: 0.8720\n",
      "Epoch 125/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5235 - acc: 0.8562 - val_loss: 0.5192 - val_acc: 0.8603\n",
      "10000/10000 [==============================] - 1s 84us/step\n",
      "\n",
      "Test result: 86.030 loss: 0.519\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    elif epoch > 100:\n",
    "        lrate = 0.0003        \n",
    "    return lrate\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Input to z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)\n",
    "\n",
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "\n",
    "# Hidden Layer 1\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), kernel_initializer='orthogonal', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Hidden Layer 2\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), kernel_initializer='orthogonal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Hidden layer 3\n",
    "model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), kernel_initializer='orthogonal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=[0.9, 1.10]\n",
    "    )\n",
    "datagen.fit(x_train)\n",
    "\n",
    "#training\n",
    "batch_size = 64\n",
    "\n",
    "mc = ModelCheckpoint('model6_weights_{epoch:08d}.h5', \n",
    "                                     save_weights_only=True, period=5)\n",
    "\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), shuffle=True,\\\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
    "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule), mc])\n",
    "\n",
    "#save to disk\n",
    "model_json = model.to_json()\n",
    "with open('model6.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model6.h5')    \n",
    "\n",
    "#testing\n",
    "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 904,
     "status": "error",
     "timestamp": 1551958551983,
     "user": {
      "displayName": "Agatha Uy",
      "photoUrl": "",
      "userId": "16915807624742537561"
     },
     "user_tz": -480
    },
    "id": "RwrEnn3S72mo",
    "outputId": "90eaface-3970-413f-ada4-9d4ec71c2a69",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "[0.5191707515716553, 0.8603]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "print(model.metrics_names)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HR1XQqym_qH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "testing_colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
